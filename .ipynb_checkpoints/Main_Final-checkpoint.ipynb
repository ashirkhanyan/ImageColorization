{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1671044461878,
     "user": {
      "displayName": "Alik Shirkhanyan",
      "userId": "01184667932271148045"
     },
     "user_tz": -240
    },
    "id": "QUHIleHI9dZq",
    "outputId": "08da7524-f88a-41f6-c924-187cae593b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Project_CS7643\n",
      "Data\t  Main_Final.ipynb  prepare-data.ipynb\n",
      "__MACOSX  output\t    project-proposal-black_white.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Change directory to the package folder\n",
    "wd = globals()['_dh'][0]\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_IUKN0ZmWAZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Pytorch package\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOEHiJ0ZUw38"
   },
   "outputs": [],
   "source": [
    "#!unzip '/content/drive/MyDrive/Project_CS7643/Data/places-data.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAjMzKusC7Gd"
   },
   "source": [
    "# Colorization part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5SIFSN2F_X-"
   },
   "source": [
    "We will experiment with **three models.** The first one will be the baseline, replicating Baldassarre et al. (2017) with the Inception V3 as our feature extractor. The second model will use the Tiny ConvNeXt as the feature extractor. Finally, the third one experiments with the fusion layer with the baseline feature extractor. Instead of the 1000 features, we will use a random sample of size 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1671029136380,
     "user": {
      "displayName": "Alik Shirkhanyan",
      "userId": "01184667932271148045"
     },
     "user_tz": -240
    },
    "id": "gSC2fFcqJBXJ",
    "outputId": "66d465ac-bf28-4235-fadd-128fe985580d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGGZcW127vA8"
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dir, model_type):\n",
    "        self.dir = dir\n",
    "        self.model_type = model_type\n",
    "        self.images = [image for image in os.listdir(self.dir)]\n",
    "        print('Number of images:', len(self.images))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # cv2 reads the image in BGR format - make sure to take this into account later\n",
    "        color_img = cv2.imread(os.path.join(self.dir,self.images[idx]))\n",
    "\n",
    "        # if we need to scale\n",
    "        color_img = color_img.astype(np.float32)\n",
    "        color_img = color_img / 255\n",
    "\n",
    "        # according to the paper by Baldassarre et al (2017), encoder images need to be of size 224x224\n",
    "        # inception images should be of size 299x299, do the same here\n",
    "        encoder_img = cv2.resize(color_img, (224, 224))\n",
    "\n",
    "        # feature extractor\n",
    "        if self.model_type == 'inception_v3' or self.model_type == 'fusion_experiment' or self.model_type == 'inception_class':\n",
    "          inception_img = cv2.resize(color_img, (299, 299))\n",
    "        else:\n",
    "          inception_img = cv2.resize(color_img, (224, 224))\n",
    "\n",
    "        # convert from BGR format to CIE L*a*b* color space\n",
    "        encoder_img_cie = cv2.cvtColor(encoder_img,cv2.COLOR_BGR2Lab)\n",
    "\n",
    "        # take the luminance channel and normalize to have a scale [-1,1]  luminance has a range from 0 to 100\n",
    "        encoder_img_lumin = encoder_img_cie[:,:,0]/50 - 1\n",
    "        # repeat the luminance channel 3 times and convert to tensor\n",
    "        encoder_img_lumin = torch.from_numpy(np.repeat(encoder_img_lumin[np.newaxis,...], 3, axis=0))\n",
    "\n",
    "        # now take the a and b channels (range from -128 to 127) and normalize them, then concatenate and convert to tensor\n",
    "        encoder_img_a = encoder_img_cie[:,:,1]/128\n",
    "        encoder_img_b = encoder_img_cie[:,:,2]/128\n",
    "        encoder_img_ab = torch.from_numpy(np.stack(([encoder_img_a, encoder_img_b]), axis=0))\n",
    "\n",
    "        # convert from BGR format to CIE L*a*b* color space - inception\n",
    "        inception_img_cie = cv2.cvtColor(inception_img,cv2.COLOR_BGR2Lab)\n",
    "\n",
    "        # again, take the luminance channel and normalize\n",
    "        inception_img_lumin = inception_img_cie[:,:,0]/50 - 1\n",
    "\n",
    "        # repeat the luminance channel of the inception image 3 times and convert to tensor\n",
    "        inception_img_lumin = torch.from_numpy(np.repeat(inception_img_lumin[np.newaxis,...], 3, axis=0))\n",
    "\n",
    "        return encoder_img_lumin, encoder_img_ab, inception_img_lumin, torch.from_numpy(encoder_img), self.images[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HKgP-IXMFIx"
   },
   "outputs": [],
   "source": [
    "# model type\n",
    "modeltype = 'inception_v3'\n",
    "# parameters\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "lr_decrease_rate = 0.5\n",
    "output_dir = wd + '/output/' + modeltype + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiGa0AWzOlX2"
   },
   "outputs": [],
   "source": [
    "# make sure all the models shuffle the data the same way\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381795,
     "status": "ok",
     "timestamp": 1671030538488,
     "user": {
      "displayName": "Alik Shirkhanyan",
      "userId": "01184667932271148045"
     },
     "user_tz": -240
    },
    "id": "C2ihBLdUL-d9",
    "outputId": "8251a026-e76f-4c34-e928-e8e72c1c717f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "Number of images: 84312\n",
      "Validation data\n",
      "Number of images: 10510\n"
     ]
    }
   ],
   "source": [
    "# get the datasets\n",
    "# initialize the data loader BEFORE the model class to make sure that the training data is shuffled the same way for all the models\n",
    "print('Training data')\n",
    "train_dataset = CustomImageDataset('Data/places-data/train', model_type = modeltype)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "print('Validation data')\n",
    "valid_dataset = CustomImageDataset('Data/places-data/valid', model_type = modeltype)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-xXeZR4HPeJ"
   },
   "outputs": [],
   "source": [
    "class FusionLayer(nn.Module):\n",
    "    def __init__(self, model_type):\n",
    "        super(FusionLayer,self).__init__()\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def forward(self, enc_img_and_emb, mask=None):\n",
    "        # img_after_enc is of shape batch_size x 256 x H/8 x W/8. H = 224, W = 224\n",
    "        # the size of the embedding out of the inception model is batch_size x 1000, in the paper the size is 1001\n",
    "        img_after_enc, embedding = enc_img_and_emb\n",
    "        # after reshape the size of the embedding should be [batch_size, 1000, 1, 1]\n",
    "        embedding=torch.reshape(embedding, (embedding.shape[0],embedding.shape[1],1,1))\n",
    "        # repeat H/8 times to have a size [batch_size, 1000, 28, 28]\n",
    "        embedding = embedding.repeat(1,1,img_after_enc.shape[2],img_after_enc.shape[3])\n",
    "        if self.model_type == 'fusion_experiment':\n",
    "          # take random 256 out of 1000\n",
    "          x = random.sample(range(1000), 256)\n",
    "          embedding = embedding[:,x,:,:]\n",
    "        # concat to have a fusion size - [batch_size, 1256, 28, 28]\n",
    "        fusion = torch.cat((img_after_enc,embedding),dim = 1)\n",
    "        return fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxYU8cndHPZ6"
   },
   "outputs": [],
   "source": [
    "class Colorize(nn.Module):\n",
    "    def __init__(self, depth_after_fusion, model_type):\n",
    "        super(Colorize,self).__init__()\n",
    "        # the encoder part\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "\n",
    "        in_chan =1256\n",
    "        if model_type == 'fusion_experiment':\n",
    "          in_chan = 512\n",
    "        # the fusion part\n",
    "        self.fusion = FusionLayer(model_type)\n",
    "        # in case of inception v3 the output of the fusion is of shape [batch_size, 1256, 28, 28], hench input channels = 1256\n",
    "        # another convolutional layer\n",
    "        self.after_fusion = nn.Conv2d(in_channels=in_chan, out_channels=depth_after_fusion,kernel_size=1, stride=1,padding=0)\n",
    "        # last Batch norm before the decoder\n",
    "        self.lastnorm = nn.BatchNorm2d(depth_after_fusion)\n",
    "        # the decoder part\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=depth_after_fusion, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2.0),\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Upsample(scale_factor=2.0),\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Upsample(scale_factor=2.0),\n",
    "        )\n",
    "    # forward pass\n",
    "    def forward(self, img_lumin, img_embed):\n",
    "        img_after_encoder = self.encoder(img_lumin)\n",
    "        fusion = self.fusion([img_after_encoder, img_embed])\n",
    "        fusion = self.after_fusion(fusion)\n",
    "        fusion = self.lastnorm(fusion)\n",
    "        pred_ab = self.decoder(fusion)\n",
    "        return pred_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVseJWe3HPQO"
   },
   "outputs": [],
   "source": [
    "# initialize the inception and transformer models\n",
    "inception_v3 = models.inception_v3(weights='DEFAULT').float().to(device)\n",
    "inception_v3.eval()\n",
    "convnext = models.convnext_tiny(weights='DEFAULT').float().to(device)\n",
    "convnext.eval()\n",
    "#transformer = models.vit_l_16(weights = 'DEFAULT').float().to(device)\n",
    "#transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQ7Gc9uzpNtg"
   },
   "outputs": [],
   "source": [
    "# set the model, Adam will be the optimizer\n",
    "model = Colorize(256, model_type = modeltype).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "# loss, decrease the learning rate every 2 epochs\n",
    "if modeltype == 'inception_class':\n",
    "  loss_criterion = torch.nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "else:\n",
    "  loss_criterion = torch.nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "# milestone to decrease\n",
    "milestone_list  = list(range(0, n_epochs ,2))\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestone_list, gamma=lr_decrease_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v269fN6XHPJ9"
   },
   "outputs": [],
   "source": [
    "# train and validate\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch:',epoch+1)\n",
    "\n",
    "    # Train\n",
    "    average_loss = 0.0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    for idx, (encoder_img_lumin, encoder_img_ab, inception_img_lumin, encoder_img, img_name) in enumerate(train_dataloader):\n",
    "\n",
    "        # send to device\n",
    "        encoder_img_lumin = encoder_img_lumin.to(device)\n",
    "        encoder_img_ab = encoder_img_ab.to(device)\n",
    "        inception_img_lumin = inception_img_lumin.to(device)\n",
    "\n",
    "        # zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # get the features from the feature extractor model\n",
    "        if modeltype == 'inception_v3' or modeltype == 'fusion_experiment' or modeltype == 'inception_class':\n",
    "          img_embed = inception_v3(inception_img_lumin.float())\n",
    "        else:\n",
    "          img_embed = convnext(inception_img_lumin.float())\n",
    "\n",
    "        pred_ab = model(encoder_img_lumin, img_embed)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss = loss_criterion(pred_ab, encoder_img_ab.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #reduce the learning rate according to the milestones\n",
    "        scheduler.step()\n",
    "\n",
    "        # calculate the losses\n",
    "        average_loss += loss.item()\n",
    "\n",
    "        if idx % 1000 == 0:\n",
    "          print('Batch: ', idx)\n",
    "\n",
    "    # Print the loss\n",
    "    train_loss = average_loss/len(train_dataloader)*batch_size\n",
    "    train_loss_history.append(train_loss)\n",
    "    print('Training Loss:',train_loss,'-------- Time passed ', round(time.time()-start, 2))\n",
    "\n",
    "    # Validation\n",
    "    average_loss = 0.0\n",
    "    start = time.time()\n",
    "    model.eval()\n",
    "    for idx,(encoder_img_lumin, encoder_img_ab, inception_img_lumin, encoder_img, img_name) in enumerate(valid_dataloader):\n",
    "\n",
    "        # send to device\n",
    "        encoder_img_lumin = encoder_img_lumin.to(device)\n",
    "        encoder_img_ab = encoder_img_ab.to(device)\n",
    "        inception_img_lumin = inception_img_lumin.to(device)\n",
    "\n",
    "        # get the features from inception v3\n",
    "        if modeltype == 'inception_v3' or modeltype == 'fusion_experiment' or modeltype == 'inception_class':\n",
    "          img_embed = inception_v3(inception_img_lumin.float())\n",
    "        else:\n",
    "          img_embed = convnext(inception_img_lumin.float())\n",
    "        pred_ab = model(encoder_img_lumin,img_embed)\n",
    "\n",
    "        # calculate the losses\n",
    "        loss = loss_criterion(pred_ab, encoder_img_ab.float())\n",
    "        average_loss += loss.item()\n",
    "\n",
    "        if idx % 1000 == 0:\n",
    "          print('Batch: ', idx)\n",
    "\n",
    "    val_loss = average_loss/len(valid_dataloader)*batch_size\n",
    "    valid_loss_history.append(val_loss)\n",
    "    print('Validation Loss:', val_loss,'-------- Time passed ', round(time.time()-start,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXCYZW3THO_A"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history, color='blue', label='Train loss')\n",
    "plt.plot(valid_loss_history, color='orange', label='Valid loss')\n",
    "plt.legend()\n",
    "plt.savefig(output_dir + modeltype + '_TrainValidLoss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arPKhM8PR7df"
   },
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset('Data/places-data/test', model_type = modeltype)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3F4_jTPSH8I"
   },
   "outputs": [],
   "source": [
    "average_loss = 0.0\n",
    "start = time.time()\n",
    "\n",
    "for idx, (encoder_img_lumin, encoder_img_ab, inception_img_lumin, encoder_img, img_name) in enumerate(test_dataloader):\n",
    "\n",
    "    # send to device\n",
    "    encoder_img_lumin = encoder_img_lumin.to(device)\n",
    "    encoder_img_ab = encoder_img_ab.to(device)\n",
    "    inception_img_lumin = inception_img_lumin.to(device)\n",
    "    #\n",
    "    model.eval()\n",
    "\n",
    "    if modeltype == 'inception_v3' or modeltype == 'fusion_experiment' or modeltype == 'inception_class':\n",
    "      img_embed = inception_v3(inception_img_lumin.float())\n",
    "    else:\n",
    "      img_embed = convnext(inception_img_lumin.float())\n",
    "    pred_ab = model(encoder_img_lumin, img_embed)\n",
    "\n",
    "    # colorize some of the test images\n",
    "    if idx % 500 == 0:\n",
    "      # remember we have repeated the luminance channel 3 times, take only one and scale back\n",
    "      lum = (encoder_img_lumin[:,0, :,:][0] + 1)*50\n",
    "      # transform to have size 1x224x224\n",
    "      lum = torch.stack([lum],dim=0)\n",
    "      # take the ab channels and scale back again, size- 2x224x224\n",
    "      ab = pred_ab[0]*128\n",
    "      # concat to get 3x224x224\n",
    "      cie = torch.cat([lum, ab], dim =0)\n",
    "      # convert to numpy and transpose to have 224x224x3\n",
    "      cie = cie.cpu().detach().numpy().transpose(1,2,0)\n",
    "      # convert to rgb and scale back\n",
    "      rgb = cv2.cvtColor(cie,cv2.COLOR_Lab2BGR) *255\n",
    "      cv2.imwrite(output_dir + modeltype + '_' + img_name[0], rgb)\n",
    "\n",
    "    # calculate the losses\n",
    "    loss = loss_criterion(pred_ab, encoder_img_ab.float())\n",
    "    average_loss += loss.item()\n",
    "test_loss = average_loss/len(test_dataloader)*1\n",
    "print('Test Loss:', test_loss,'-------- Time passed ', round(time.time()-start,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94QxeAI11eKX"
   },
   "outputs": [],
   "source": [
    "with open(output_dir + modeltype + '.txt', 'w') as f:\n",
    "  f.write('The test loss: %f' % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k85UoUwkJbCX"
   },
   "outputs": [],
   "source": [
    "# take the ground truth images and make them grayscale, then save\n",
    "gt_dir = wd + '/output/ground_truth/'\n",
    "gr_dir = wd + '/output/grayscale/'\n",
    "\n",
    "gt = os.listdir(gt_dir)\n",
    "for image in gt:\n",
    "  img = cv2.imread(gt_dir + image)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imwrite(gr_dir + 'grayscale_' + image, img)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
